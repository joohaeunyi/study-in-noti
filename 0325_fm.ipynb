{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885081f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class dataloader:\n",
    "    def __init__(self,datapath):\n",
    "        self.datapath =datapath\n",
    "        ratings_df = pd.read_csv(os.path.join(self.datapath,\"ratings.csv\"),encoding='utf-8')\n",
    "        ratings_df.drop('timestamp', inplace=True, axis=1)\n",
    "\n",
    "        movies_df = pd.read_csv(os.path.join(self.datapath, \"movies.csv\"), encoding='utf-8')\n",
    "        movies_df = movies_df.set_index(\"movieId\")\n",
    "        dummy_genre_df =  movies_df['genres'].str.get_dummies(sep='|')\n",
    "\n",
    "\n",
    "        movies_df['year'] = movies_df[\"title\"].str.extract('(\\(\\d\\d\\d\\d\\))')\n",
    "        movies_df['year'] = movies_df['year'].astype('str')\n",
    "        movies_df['year'] = movies_df['year'].map(lambda x: x.replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "        movies_df.loc[movies_df['year'] =='nan', 'year'] = '1980'        \n",
    "        movies_df['year'] = movies_df['year'].astype(\"float32\").astype(\"int32\")\n",
    "        movies_df.drop(movies_df[movies_df['year'] == 0].index, inplace=True, axis=0)\n",
    "        movies_df.drop('title',axis=1,inplace=True)\n",
    "        bins = list(range(1900, 2021, 20))\n",
    "        labels = [x for x in range(len(bins) - 1)]\n",
    "        movies_df['year_level'] = pd.cut(movies_df['year'], bins, right=False, labels=labels)\n",
    "        movies_df.drop('year', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "        threshold = 10\n",
    "        over_threshold = ratings_df.groupby('movieId').size() >= threshold\n",
    "        ratings_df['over_threshold'] = ratings_df['movieId'].map(lambda x: over_threshold[x])\n",
    "        ratings_df = ratings_df[ratings_df[\"over_threshold\"] == True]\n",
    "        ratings_df.drop(\"over_threshold\", axis=1, inplace=True)\n",
    "        random_idx = np.random.permutation(len(ratings_df))\n",
    "        shuffled_df = ratings_df.iloc[random_idx]\n",
    "        shuffled_df.to_csv('shuffled_df.csv', index=False)\n",
    "        concat_df = pd.concat([\n",
    "            pd.get_dummies(shuffled_df['userId'], prefix=\"user\"),\n",
    "            pd.get_dummies(shuffled_df['movieId'], prefix=\"movie\"),\n",
    "            shuffled_df['movieId'].apply(lambda x: dummy_genre_df.loc[x]),\n",
    "            shuffled_df['movieId'].apply(lambda x: movies_df.loc[x][\"year_level\"]).rename('year_level'),\n",
    "        ], axis=1)\n",
    "\n",
    "        target_df = ratings_df.loc[concat_df.index]['rating']\n",
    "        target_df = target_df.apply(lambda x: 1 if x >= 4 else 0)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(concat_df, target_df, test_size=0.1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(print(\"---dataloader---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd03e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"NeuralMF.\")\n",
    "    parser.add_argument('--path', nargs='?', default='/dataset/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--dataset', nargs='?', default='ratings.csv',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--num_factors', type=int, default=8,help='latent feature of FM model.')\n",
    "    parser.add_argument('--epochs', type=int, default=10,help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,help='Batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "class FM(keras.Model):\n",
    "    def __init__(self, n_factor=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.w_0 = tf.Variable([0.0])\n",
    "        self.w = tf.Variable(tf.zeros(shape=[p]))\n",
    "        self.v = tf.Variable(tf.random.normal(shape=(p, n_factor)))\n",
    "\n",
    "    def call(self,inputs):\n",
    "        degree_1 = tf.reduce_sum(tf.multiply(self.w, inputs), axis=1)\n",
    "\n",
    "        degree_2 = 0.5 * tf.reduce_sum(\n",
    "            tf.math.pow(tf.matmul(inputs, self.v), 2)\n",
    "            - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.v, 2))\n",
    "            , 1\n",
    "            , keepdims=False\n",
    "        )\n",
    "\n",
    "        predict = tf.math.sigmoid(self.w_0 + degree_1 + degree_2)\n",
    "\n",
    "        return predict\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(f\"\\r{iteration}/{total}  \" + metrics ,\n",
    "          end = end)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    '''\n",
    "    args = parse_args()\n",
    "    print(args)\n",
    "    num_factors = args.num_factors\n",
    "    learner = args.learner\n",
    "    learning_rate = args.lr\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    '''\n",
    "    num_factors = 8\n",
    "    learner = 'adam'\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    loader =dataloader('dataset')\n",
    "    X_train =loader.X_train\n",
    "    y_train = loader.y_train\n",
    "    X_test = loader.X_test\n",
    "    y_test = loader.y_test\n",
    "\n",
    "    n = X_train.shape[0]\n",
    "    p = X_train.shape[1]\n",
    "    \n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "\n",
    "    n_steps = len(X_train) // batch_size\n",
    "\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        optimizer=keras.optimizers.Adagrad(lr=learning_rate)\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        optimizer=keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    elif learner.lower() == \"adam\":\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "    loss_fn = keras.losses.binary_crossentropy\n",
    "    mean_loss = keras.metrics.Mean()\n",
    "    metrics = [keras.metrics.BinaryAccuracy()]\n",
    "    test_acc = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    model = FM(n_factor=num_factors)\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(X_train, tf.float32), tf.cast(y_train, tf.float32))).shuffle(500).batch(batch_size)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(X_test, tf.float32), tf.cast(y_test, tf.float32))).shuffle(200).batch(batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"에포크 : {epoch}/{epochs}\")\n",
    "\n",
    "        for step, (X_batch, y_batch) in enumerate(train_data):\n",
    "            # train, test data\n",
    "            with tf.GradientTape() as tape:\n",
    "                predict = model(X_batch)\n",
    "                loss = loss_fn(y_batch, predict)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            mean_loss(loss)\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric(y_batch, predict)\n",
    "\n",
    "            print_status_bar(step * batch_size, len(y_train), mean_loss, metrics=metrics)\n",
    "\n",
    "        for x_test, y_test in test_data:\n",
    "            prediction = model(x_test)\n",
    "            test_acc.update_state(y_test, prediction)\n",
    "\n",
    "        print_status_bar(n_steps * batch_size, n_steps * batch_size, mean_loss, metrics=metrics)\n",
    "        print(\"검증 정확도: \", test_acc.result().numpy())\n",
    "        for metric in [mean_loss] + [test_acc] +metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(x_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in prediction:\n",
    "    print(round(i.numpy(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15926155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
